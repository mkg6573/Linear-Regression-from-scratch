# Linear Regression From Scratch ğŸ“ˆ

This project implements **Linear Regression from scratch using Python**, without using machine learning libraries like `scikit-learn`.  
The goal is to understand the **mathematics and logic behind linear regression**, including cost function optimization using **Gradient Descent**.

ğŸš€ Live Project Deployment

ğŸ”— Streamlit Web App:
ğŸ‘‰ https://mobile-priceprediction-004.streamlit.app/

This web application allows users to input mobile specifications and get real-time price predictions using the trained Multiple Linear Regression model.


## ğŸ“Š Mobile Price Prediction Dataset

This dataset is designed to analyze how various smartphone specifications influence mobile prices and overall device value. It provides a structured view of hardware features, software attributes, and connectivity options, making it ideal for **data analysis**, **feature engineering**, and **machine learning model building**.

Although this is not an official commercial dataset, it is well-organized and informative. It helps in developing analytical thinking, understanding feature impact, and gaining hands-on experience with real-world-like data for both academic and professional learning.



## ğŸ“Œ Key Features / Columns

* **brand** â€“ Mobile phone brand
* **price** â€“ Device price
* **rating** â€“ User rating score
* **is_5g** â€“ 5G network support (Yes/No)
* **is_nfc** â€“ NFC support
* **is_ir_blaster** â€“ IR blaster availability
* **processor_brand** â€“ Processor manufacturer
* **core** â€“ Number of processor cores
* **processor_speed** â€“ Processor speed
* **ram** â€“ RAM capacity
* **internal_memory** â€“ Internal storage size
* **battery_size** â€“ Battery capacity
* **fast_charge** â€“ Fast charging support
* **charging_speed** â€“ Charging speed
* **rear_mp** â€“ Rear camera megapixels
* **front_mp** â€“ Front camera megapixels
* **os** â€“ Operating system
* **display_size** â€“ Screen size
* **refresh_rate** â€“ Display refresh rate
* ......

## ğŸ“Š Models Implemented

1. Simple Linear Regression  
2. Multiple Linear Regression  
3. Polynomial Regression  

---

## ğŸ“ˆ Model Performance Comparison

### ğŸ”¹ 1. Simple Linear Regression
ğŸ“Œ Objective

Notebook : https://github.com/mkg6573/Linear-Regression-from-scratch/blob/main/Simple_Linear_Regression.ipynb

* The objective of this experiment is to implement Simple Linear Regression from scratch using Gradient Descent to model the linear relationship between a single independent variable and the target variable.

ğŸ“Š Dataset Description

Dataset Name: after_EDA_dataset.csv

Independent Variable (Feature): processor_speed

Dependent Variable (Target): price

The dataset was cleaned and preprocessed after Exploratory Data Analysis (EDA).

ğŸ§  Methodology
--
1ï¸âƒ£ Data Preprocessing
-
Fill value : https://github.com/mkg6573/Linear-Regression-from-scratch/blob/main/fill-NAN-value.ipynb

EDA : https://github.com/mkg6573/Linear-Regression-from-scratch/blob/main/EDA.ipynb

corelation : https://github.com/mkg6573/Linear-Regression-from-scratch/blob/main/Correlation.ipynb
Loaded dataset using Pandas

Extracted feature (X) and target (y)

Reshaped arrays appropriately

2ï¸âƒ£ Model Initialization
-
Parameters initialized:

Î¸â‚€ (Intercept)

Î¸â‚ (Slope)

Learning rate defined

Number of epochs specified

3ï¸âƒ£ Gradient Descent Implementation
-
Predictions calculated using:
          y=Î¸0â€‹+Î¸1â€‹x

Cost function computed using Mean Squared Error (MSE)

Gradients derived manually

Parameters updated iteratively

4ï¸âƒ£ Training Monitoring
-
Cost recorded per epoch

Verified convergence via decreasing loss

ğŸ§® Model Equation

y=Î¸0â€‹+Î¸1â€‹x


Where:
x = processor speed
y = price

ğŸ“ˆ Evaluation Metrics

- RÂ² Score: **0.3869**
- MAE: **9324.92**
- RMSE: **17917.85**

ğŸ“Œ Interpretation:
- Explains only 38% of variance in price.
- High error values indicate underfitting.
- Not suitable for complex mobile pricing patterns.

ğŸ“Š Results & Observations

The cost function decreases steadily.

The model captures the linear relationship between processor speed and price.

Works well if the relationship is approximately linear.

ğŸ Conclusion

Simple Linear Regression was successfully implemented from scratch using Gradient Descent, demonstrating the fundamental working of supervised learning without external ML libraries.

---

### ğŸ”¹ 2. Multiple Linear Regression
ğŸ“Œ Objective
Notebook : https://github.com/mkg6573/Linear-Regression-from-scratch/blob/main/Multiple_Linear_Regression.ipynb
To implement Multiple Linear Regression from scratch using Gradient Descent to model the relationship between multiple features and price.

ğŸ“Š Dataset Description

* Dataset Name: after_EDA_dataset.csv
* Independent Variables:
* processor_speed
* RAM
* storage
* (other selected features)

Dependent Variable: price
ğŸ§  Methodology
1ï¸âƒ£ Data Preparation

* Selected multiple relevant features
* Constructed feature matrix (X)
* Added bias column (ones column)

2ï¸âƒ£ Model Initialization

* Parameter vector Î¸ initialized to zeros
* Learning rate defined
* Epochs defined

3ï¸âƒ£ Vectorized Gradient Descent

Predictions computed using: y=XÎ¸
Cost function computed using vectorized MSE

Gradient computed using matrix operations

Parameters updated simultaneously

4ï¸âƒ£ Training Monitoring

Cost tracked over epochs

Convergence confirmed through decreasing loss

ğŸ§® Model Equation
         y=Î¸0â€‹+Î¸1â€‹x1â€‹+Î¸2â€‹x2â€‹+...+Î¸nâ€‹xnâ€‹

ğŸ“ˆ Evaluation Metrics
- RÂ² Score: **0.8416**
- MAE: **4963.06**
- RMSE: **9108.06**

ğŸ“Œ Interpretation:
- Explains 84% of variance in price.
- Significant improvement over Simple Linear Regression.
- Much lower prediction error.
- Best performing model among all tested models.

ğŸ“Š Results & Observations

Model captures influence of multiple features on price.

Better predictive performance than Simple Linear Regression.

Handles multivariate relationships effectively.

ğŸ Conclusion

Multiple Linear Regression was successfully implemented using fully vectorized Gradient Descent, improving predictive accuracy by leveraging multiple input features
---

### ğŸ”¹ 3. Polynomial Regression
ğŸ“Œ Objective

Notebook : https://github.com/mkg6573/Linear-Regression-from-scratch/blob/main/Polynomial_Regression.ipynb

The objective of this experiment is to implement Polynomial Regression from scratch using Gradient Descent to model the non-linear relationship between processor speed and price without using any pre-built machine learning libraries.

ğŸ“Š Dataset Description

Dataset Name: after_EDA_dataset.csv

Independent Variable (Feature): processor_speed

Dependent Variable (Target): price

The dataset was cleaned and preprocessed after performing Exploratory Data Analysis (EDA).

ğŸ§  Methodology
1ï¸âƒ£ Data Preparation

Dataset loaded using Pandas

Extracted feature (X) and target (y)

Reshaped arrays where necessary

2ï¸âƒ£ Feature Engineering

*Created polynomial feature of degree 2 manually:
                    ğ‘¥2
*Constructed the Design Matrix (X):
   *Bias term (1)
   *Linear term (x)
   *Polynomial term (xÂ²)

3ï¸âƒ£ Model Initialization

*Parameters (Î¸â‚€, Î¸â‚, Î¸â‚‚) initialized to zero
*Learning rate defined
*Number of epochs defined

4ï¸âƒ£ Gradient Descent Implementation

Predicted output calculated using matrix multiplication

Cost function computed using Mean Squared Error

Gradients calculated manually

Parameters updated iteratively

5ï¸âƒ£ Monitoring Training

Cost recorded for each epoch

Convergence verified through decreasing loss

ğŸ§® Model Equation

The Polynomial Regression model used:
          y=Î¸0â€‹+Î¸1â€‹x+Î¸2â€‹x2
          Where:

ğ‘¥= processor speed
ğ‘¦= price
ğœƒ0= intercept
Î¸1= linear coefficient
Î¸2= polynomial coefficient

ğŸ“ˆ Evaluation Metrics
- MSE: **547017769.79**
- RMSE: **23388.41**
- RÂ² Score: **0.4365**

ğŸ“Œ Interpretation:
- Performance dropped compared to Multiple Linear Regression.
- Likely overfitting or improper polynomial degree selection.
- Not suitable for this dataset in current configuration.

ğŸ“Š Results & Observations

The cost function decreases steadily, indicating successful convergence.

Polynomial Regression captures non-linear patterns more effectively than Simple Linear Regression.

The regression curve better fits the curvature of the data.
ğŸ Conclusion

Polynomial Regression was successfully implemented from scratch using Gradient Descent. By introducing higher-order terms, the model effectively captured non-linear relationships and improved prediction accuracy.

---

### ğŸ”¹ 4. Regularization (Ridge & Lasso)
ğŸ“Œ Objective
Notebook : https://github.com/mkg6573/Linear-Regression-from-scratch/blob/main/Regularization.ipynb
-To reduce overfitting and improve model stability.
-Ridge Regression (L2) shrinks large coefficients.
-Lasso Regression (L1) performs feature selection.
-Regularization stabilized coefficients but did not outperform Multiple Linear Regression.

ğŸ§  Methodology
ğŸ”¹ Ridge Regression (L2 Regularization)

Modified cost function:
       J(Î¸)=1/2mâ€‹âˆ‘(hÎ¸â€‹(x)âˆ’y)^2+Î»/2mâ€‹âˆ‘Î¸^2
Penalizes large coefficients.
Shrinks weights but does not eliminate them.

ğŸ”¹ Lasso Regression (L1 Regularization)
      J(Î¸)=1/2mâ€‹âˆ‘(hÎ¸â€‹(x)âˆ’y)^2+Î»/mâ€‹âˆ‘âˆ£Î¸âˆ£
-Can reduce some coefficients to zero.
-Performs feature selection. 

### ğŸ”¹ 5.  Model Comparison & Final Analysis
ğŸ“Š Final Performance Comparison

| Model                     | RÂ² Score | RMSE      | Performance |
|----------------------------|----------|-----------|------------|
| Simple Linear Regression   | 0.38     | 17917     | âŒ Poor     |
| Multiple Linear Regression | 0.84     | 9108      | âœ… Best     |
| Polynomial Regression      | 0.43     | 23388     | âŒ Weak     |

âœ… **Multiple Linear Regression gives the best results with 84% accuracy.**

---
ğŸ† Final Conclusion

âœ… Multiple Linear Regression gives the best results with 84% accuracy.

It explains most of the variance in mobile prices.

It significantly reduces prediction error.

It captures complex pricing relationships effectively.

Simple Linear Regression underfits the data, while Polynomial Regression did not improve performance in this case.

Therefore, Multiple Linear Regression was selected for deployment in the Streamlit web application.

## ğŸ›  Technologies Used

- Python
- NumPy
- Pandas
- Matplotlib

---


---

## ğŸ“Œ Project Objective

To understand how different regression algorithms perform on real-world pricing data and build strong intuition about model evaluation metrics like:

- RÂ² Score
- MAE
- RMSE
- MSE
---

ğŸŒ Deployment Details
ğŸ”¹ Model File
phone_price_model.pkl

ğŸ”¹ Streamlit App
app.py

ğŸ”¹ Prediction Logic
*Model trained on log1p(price)
*Predictions converted back using:
*predicted_price = np.expm1(log_price)

ğŸ”¹ User Inputs in Web App
Brand
Processor Type
RAM
Storage
Camera
Battery
Display
5G Support
NFC
Charging Speed
etc.

ğŸ›  Tech Stack
-Python
-NumPy
-Pandas
-Matplotlib
-Scikit-learn
-Joblib
-Streamlit

ğŸ“ Learning Outcomes
-
Through this project, we gained hands-on experience in:
End-to-end regression modeling
Feature engineering
Model evaluation (MSE, RMSE, RÂ²)
Overfitting and regularization
Model comparison and selection
Deployment using Streamlit
Converting ML models into real-world applications

ğŸ Conclusion
-
This project demonstrates a complete machine learning pipeline:
EDA â†’ Model Building â†’ Model Evaluation â†’ Model Comparison â†’ Model Selection â†’ Deployment
Among all tested models, Multiple Linear Regression provided the best balance between accuracy and generalization, achieving an RÂ² score of 0.84 and the lowest RMSE.
The model is successfully deployed as a live web application for real-time mobile price prediction.

â­ If you like this project, consider giving it a star on GitHub!
